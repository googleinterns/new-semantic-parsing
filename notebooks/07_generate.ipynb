{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from new_semantic_parsing import EncoderDecoderWPointerModel, Seq2SeqTrainer\n",
    "from new_semantic_parsing.schema_tokenizer import TopSchemaTokenizer\n",
    "from new_semantic_parsing.utils import compute_metrics, get_src_pointer_mask\n",
    "from new_semantic_parsing.data import PointerDataset, Seq2SeqDataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "vocab = {'[', ']', 'IN:', 'SL:', 'GET_DIRECTIONS', 'DESTINATION',\n",
    "         'DATE_TIME_DEPARTURE', 'GET_ESTIMATED_ARRIVAL'}\n",
    "schema_tokenizer = TopSchemaTokenizer(vocab, tokenizer)\n",
    "\n",
    "source_texts = [\n",
    "    'Directions to Lowell',\n",
    "    'Get directions to Mountain View',\n",
    "]\n",
    "schema_texts = [\n",
    "    '[IN:GET_DIRECTIONS Directions to [SL:DESTINATION Lowell]]',\n",
    "    '[IN:GET_DIRECTIONS Get directions to [SL:DESTINATION Mountain View]]'\n",
    "]\n",
    "\n",
    "source_ids = [tokenizer.encode(t) for t in source_texts]\n",
    "source_pointer_masks = [get_src_pointer_mask(i, tokenizer) for i in source_ids]\n",
    "\n",
    "schema_ids = []\n",
    "schema_pointer_masks = []\n",
    "\n",
    "for src_id, schema in zip(source_ids, schema_texts):\n",
    "    item = schema_tokenizer.encode_plus(schema, src_id)\n",
    "    schema_ids.append(item.ids)\n",
    "    schema_pointer_masks.append(item.pointer_mask)\n",
    "\n",
    "dataset = PointerDataset(source_ids, schema_ids, source_pointer_masks, schema_pointer_masks)\n",
    "dataset.torchify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101, 17055,  1116,  1106, 16367,   102]),\n",
       " 'decoder_input_ids': tensor([ 1,  9,  7,  5, 12, 13, 14,  9,  8,  4, 15, 10, 10]),\n",
       " 'attention_mask': None,\n",
       " 'decoder_attention_mask': None,\n",
       " 'pointer_mask': tensor([0., 1., 1., 1., 1., 0.]),\n",
       " 'decoder_pointer_mask': tensor([0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0.]),\n",
       " 'labels': tensor([ 9,  7,  5, 12, 13, 14,  9,  8,  4, 15, 10, 10,  2])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dec1593b36543adaa7454b9cc282a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=1.0, style=ProgressStyle(description_widâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\"eval_loss\": 0.12566250562667847, \"eval_accuracy\": 1.0, \"eval_exact_match\": 1.0, \"epoch\": 30.0, \"step\": 30}\n"
     ]
    }
   ],
   "source": [
    "src_maxlen, _ = dataset.get_max_len()\n",
    "\n",
    "model = EncoderDecoderWPointerModel.from_parameters(\n",
    "    layers=3, hidden=128, heads=2, max_src_len=src_maxlen,\n",
    "    src_vocab_size=tokenizer.vocab_size, tgt_vocab_size=schema_tokenizer.vocab_size\n",
    ")\n",
    "\n",
    "train_args = transformers.TrainingArguments(\n",
    "    output_dir='output_dir',\n",
    "    do_train=True,\n",
    "    num_train_epochs=30,\n",
    "    seed=42,\n",
    "    learning_rate=1e-3,\n",
    ")\n",
    "\n",
    "# doesn't work, patch transformers?\n",
    "transformers.trainer.is_wandb_available = lambda: False  # workaround\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    train_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=Seq2SeqDataCollator(model.encoder.embeddings.word_embeddings.padding_idx),\n",
    "    eval_dataset=dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "# a trick to reduce the amount of logging\n",
    "trainer.is_local_master = lambda: False\n",
    "\n",
    "# random.seed(42)\n",
    "# torch.manual_seed(42)\n",
    "# np.random.seed(42)\n",
    "\n",
    "train_out = trainer.train()\n",
    "eval_out = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101, 17055,  1116,  1106, 16367,   102])\n",
      "tensor([ 1,  9,  7,  5, 12, 13, 14,  9,  8,  4, 15, 10, 10])\n",
      "tensor([ 9,  7,  5, 12, 13, 14,  9,  8,  4, 15, 10, 10,  2])\n",
      "\n",
      "[CLS] Directions to Lowell [SEP]\n",
      "[BOS] [IN:GET_DIRECTIONS Directions to [SL:DESTINATION Lowell ] ]\n",
      "[IN:GET_DIRECTIONS Directions to [SL:DESTINATION Lowell ] ] [EOS]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0].input_ids)\n",
    "print(dataset[0].decoder_input_ids)\n",
    "print(dataset[0].labels)\n",
    "\n",
    "print()\n",
    "print(tokenizer.decode(dataset[0].input_ids))\n",
    "print(schema_tokenizer.decode(dataset[0].decoder_input_ids, dataset[0].input_ids))\n",
    "print(schema_tokenizer.decode(dataset[0].labels, dataset[0].input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, collate_fn=Seq2SeqDataCollator(model.encoder.embeddings.word_embeddings.padding_idx).collate_batch\n",
    ")\n",
    "\n",
    "batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'decoder_input_ids', 'pointer_mask', 'decoder_pointer_mask', 'labels', 'attention_mask', 'decoder_attention_mask'])\n",
      "tensor([[  101, 17055,  1116,  1106, 16367,   102,     0],\n",
      "        [  101,  3949,  7768,  1106,  3757, 10344,   102]])\n",
      "tensor([[ 1,  9,  7,  5, 12, 13, 14,  9,  8,  4, 15, 10, 10,  0],\n",
      "        [ 1,  9,  7,  5, 12, 13, 14,  9,  8,  4, 15, 16, 10, 10]])\n",
      "\n",
      "tensor([[ 9,  7,  5, 12, 13, 14,  9,  8,  4, 15, 10, 10,  2,  0],\n",
      "        [ 9,  7,  5, 12, 13, 14,  9,  8,  4, 15, 16, 10, 10,  2]])\n"
     ]
    }
   ],
   "source": [
    "print(batch.keys())\n",
    "print(batch['input_ids'])\n",
    "print(batch['decoder_input_ids'])\n",
    "print()\n",
    "print(batch['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9,  7,  5, 12, 13, 14,  9,  8,  4, 15, 10, 10,  2,  2],\n",
       "        [ 9,  7,  5, 12, 13, 14,  9,  8,  4, 15, 16, 10, 10,  2]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(**batch)\n",
    "logits = out[1]\n",
    "\n",
    "logits.max(-1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9,  7,  5, 12, 13, 14,  9,  8,  4, 15, 10, 10, 10,  2],\n",
      "        [ 9,  7,  5, 12, 13, 14,  9,  8,  4, 15, 16, 10, 10,  2]])\n",
      "[IN:GET_DIRECTIONS Directions to [SL:DESTINATION Lowell ] ] ] [EOS]\n",
      "[IN:GET_DIRECTIONS Get directions to [SL:DESTINATION Mountain View ] ] [EOS]\n"
     ]
    }
   ],
   "source": [
    "out = model(input_ids=batch['input_ids'], decoder_input_ids=batch['decoder_input_ids'])\n",
    "logits = out[0]\n",
    "\n",
    "print(logits.max(-1).indices)\n",
    "print(schema_tokenizer.decode(logits.max(-1).indices[0], batch['input_ids'][0]))\n",
    "print(schema_tokenizer.decode(logits.max(-1).indices[1], batch['input_ids'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9,  7,  5, 12, 13, 14,  9,  8,  4, 15, 10, 10, 10,  2],\n",
      "        [ 9,  7,  5, 12, 13, 14,  9,  8,  4, 15, 16, 10, 10,  2]])\n",
      "[IN:GET_DIRECTIONS Directions to [SL:DESTINATION Lowell ] ] ] [EOS]\n",
      "[IN:GET_DIRECTIONS Get directions to [SL:DESTINATION Mountain View ] ] [EOS]\n"
     ]
    }
   ],
   "source": [
    "out = model(input_ids=batch['input_ids'], decoder_input_ids=batch['decoder_input_ids'], pointer_mask=batch['pointer_mask'])\n",
    "logits = out[0]\n",
    "\n",
    "print(logits.max(-1).indices)\n",
    "print(schema_tokenizer.decode(logits.max(-1).indices[0], batch['input_ids'][0]))\n",
    "print(schema_tokenizer.decode(logits.max(-1).indices[1], batch['input_ids'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IN:GET_DIRECTIONS Directions to [SL:DESTINATION Lowell ] ] ] [EOS]\n"
     ]
    }
   ],
   "source": [
    "_ = schema_tokenizer.decode(logits.max(-1).indices[0], batch['input_ids'][0].numpy())\n",
    "print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IN:GET_DIRECTIONS Get directions to [SL:DESTINATION Mountain View ] ] [EOS]\n"
     ]
    }
   ],
   "source": [
    "_ = schema_tokenizer.decode(logits.max(-1).indices[1], batch['input_ids'][1].numpy())\n",
    "print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      "tensor([  101, 17055,  1116,  1106, 16367,   102])\n",
      "[CLS] Directions to Lowell [SEP]\n",
      "\n",
      "Expected: \n",
      "\n",
      "tensor([ 9,  7,  5, 12, 13, 14,  9,  8,  4, 15, 10, 10,  2,  2])\n",
      "[IN:GET_DIRECTIONS Directions to [SL:DESTINATION Lowell ] ] [EOS] [EOS]\n"
     ]
    }
   ],
   "source": [
    "example = dataset[0]\n",
    "input_ids = example.input_ids\n",
    "labels = example.labels\n",
    "pointer_mask = example.pointer_mask\n",
    "max_len = len(example.decoder_input_ids)\n",
    "\n",
    "print('Input: ')\n",
    "print(input_ids)\n",
    "print(tokenizer.decode(input_ids))\n",
    "print()\n",
    "print('Expected: ')\n",
    "print()\n",
    "\n",
    "generated = model.generate(\n",
    "    input_ids=input_ids.unsqueeze(0),\n",
    "#     attention_mask=...,  # for batched decoding\n",
    "    max_length=max_len+2,\n",
    "    num_beams=4,\n",
    "#     pad_token_id=tokenizer.pad_token_id,\n",
    "    bos_token_id=schema_tokenizer.bos_token_id,\n",
    "#     eos_token_id=schema_tokenizer.eos_token_id,\n",
    "#     model_specific_kwargs,  # just in case\n",
    "    pointer_mask=pointer_mask.unsqueeze(0),\n",
    ").squeeze()\n",
    "\n",
    "decoded = schema_tokenizer.decode(generated, input_ids)\n",
    "\n",
    "print\n",
    "print(generated)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      "tensor([[  101, 17055,  1116,  1106, 16367,   102,     0],\n",
      "        [  101,  3949,  7768,  1106,  3757, 10344,   102]])\n",
      "[CLS] Directions to Lowell [SEP] [PAD]\n",
      "\n",
      "Expected: \n",
      "tensor([ 9,  7,  5, 12, 13, 14,  9,  8,  4, 15, 10, 10,  2,  0])\n",
      "[IN:GET_DIRECTIONS Directions to [SL:DESTINATION Lowell ] ] [EOS] [PAD]\n",
      "\n",
      "\n",
      "tensor([[ 9,  7,  5, 12, 13, 14,  9,  8,  4, 15, 10, 10,  2,  2, 10],\n",
      "        [ 9,  7,  5, 12, 13, 14,  9,  8,  4, 15, 16, 10, 10,  2, 10]])\n",
      "[IN:GET_DIRECTIONS Directions to [SL:DESTINATION Lowell ] ] [EOS] [EOS] ]\n"
     ]
    }
   ],
   "source": [
    "# batched generation\n",
    "\n",
    "example = next(iter(dl))\n",
    "input_ids = example['input_ids']\n",
    "labels = example['labels']\n",
    "pointer_mask = example['pointer_mask']\n",
    "max_len = max(map(len, example['decoder_input_ids']))\n",
    "\n",
    "print('Input: ')\n",
    "print(input_ids)\n",
    "print(tokenizer.decode(input_ids[0]))\n",
    "print()\n",
    "print('Expected: ')\n",
    "print(labels[0])\n",
    "print(schema_tokenizer.decode(labels[0], input_ids[0]))\n",
    "print()\n",
    "\n",
    "generated = model.generate(\n",
    "    input_ids=input_ids,\n",
    "#     attention_mask=...,  # for batched decoding\n",
    "    max_length=max_len+2,\n",
    "#     num_beams=4,\n",
    "#     pad_token_id=tokenizer.pad_token_id,\n",
    "    bos_token_id=schema_tokenizer.bos_token_id,\n",
    "#     eos_token_id=schema_tokenizer.eos_token_id,\n",
    "#     model_specific_kwargs,  # just in case\n",
    "    pointer_mask=pointer_mask,\n",
    ").squeeze()\n",
    "\n",
    "decoded = schema_tokenizer.decode(generated[0], input_ids[0])\n",
    "\n",
    "print()\n",
    "print(generated)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 0., 0.],\n",
       "        [0., 1., 1., 1., 1., 0., 0.],\n",
       "        [0., 1., 1., 1., 1., 0., 0.],\n",
       "        [0., 1., 1., 1., 1., 0., 0.],\n",
       "        [0., 1., 1., 1., 1., 1., 0.],\n",
       "        [0., 1., 1., 1., 1., 1., 0.],\n",
       "        [0., 1., 1., 1., 1., 1., 0.],\n",
       "        [0., 1., 1., 1., 1., 1., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointer_mask.repeat_interleave(repeats=4, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
